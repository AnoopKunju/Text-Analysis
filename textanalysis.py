# -*- coding: utf-8 -*-
"""TextAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QLqqkteD4NK0RukFI_2NNXxxDNz_22Mg

#### Text Analytics

***a) Preprocessing the data <br>
b) Performing unsupervised learning algorithms on the data: Multidimensional Scaling, Hierarchical Clustering, and K-Means to find do they yield similar results.<br>
c) Performing Topic Modeling: Obtain 5 topics using three different algorithms, namely, NMF, LSA/LSI, and LDA.***
"""

import os
import string
import pandas as pd
import glob as glob
from sklearn.manifold import MDS
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import spacy
import nltk
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import cosine_similarity
from scipy.cluster.hierarchy import ward, dendrogram
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import NMF
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from sklearn.decomposition import TruncatedSVD

nlp = spacy.load("en_core_web_sm")

"""***Loading Data***"""

os.chdir('D:\masters\DATASCIENCE\Projects\Text Analysis\dataset\Patents_xls')
files = glob.glob('*xlsx')
files

for f in files:
    df = pd.read_excel(f)
    #preprocessing to remove NULL value in the following columns of title and abstract
    df["TI"].fillna("", inplace = True)
    df["AB"].fillna("", inplace = True)
    #converting the dataframe to list to further process 
    titles = list(df['TI'])
    abstracts = list(df['AB'])
    all_text= [i + j for i, j in zip(titles, abstracts)]
    #joining all the list elements with a space into a single text file
    final_text= " ".join(all_text)
    #saving to a local file
    filename = "D:\masters\DATASCIENCE\Projects\Text Analysis\dataset\Preprocessed_txt\ " +f.split(".")[0]+ ".txt"
    with open(filename, "w+", encoding='utf-8', errors='ignore') as outfile:
        outfile.write(final_text)

os.chdir('D:\masters\DATASCIENCE\Projects\Text Analysis\dataset\Preprocessed_txt')
txt_files = glob.glob('*txt')
txt_files = [x.strip() for x in txt_files]  #remvoing space in list
txt_files

"""***Creating Corpus***"""

path = 'D:\masters\DATASCIENCE\Projects\Text Analysis\dataset\Preprocessed_txt\ '
corpus = []
for fname in txt_files:
    with open(path + fname, 'r', encoding='utf-8', errors='ignore') as textfile:
        corpus.append(textfile.read())
len(corpus)

"""***Data Preprocessing***"""

def perprocess(text):
    nlp.max_length = len(text)
    stopwords = nltk.corpus.stopwords.words("english")
    # Preprocessing by removing Uppercase and removing the punctuation & digits
    raw_text = text.lower() #coverting to lower case
    exclude_list = string.digits + string.punctuation #removing the punctuation & digits
    # Lemmatization
    table = str.maketrans(exclude_list,len(exclude_list)*" ")
    raw_text = raw_text.translate(table)
    doc = nlp(raw_text, disable = ['ner', 'parser']) #Loading into the 'en model' Lemmatization
    lemmatized_output = " ".join([token.lemma_ for token in doc]) #Lemmatize list of words and join
    # Removing Stopwords
    words = lemmatized_output.split()
    clean_text = " ".join([w for w in words if w not in stopwords])
    return clean_text

clean_corpus =[]
for text in corpus:
    clean_corpus.append(perprocess(text))
    clean_corpus=list(map(lambda st: str.replace(st,'-PRON-',''), clean_corpus)) #removing string -PRON- 
clean_corpus

len(clean_corpus) # Confirming the data is not lost and corpus is created correctly

"""### Performing Unsupervised Learning"""

vectorizer = TfidfVectorizer(stop_words = 'english', min_df = 2)
dtm = vectorizer.fit_transform(clean_corpus)
similarity = cosine_similarity(dtm)
cos_distance = 1 - similarity #cosine distance

"""***Multidimensional Scaling Algorithm***"""

mds = MDS(n_components = 2, dissimilarity='precomputed', random_state=999)
pos = mds.fit_transform(cos_distance)
xs, ys = pos[:,0], pos[:,1]
os.chdir('D:\masters\DATASCIENCE\Projects\Text Analysis\dataset\Patents_xls')
files = glob.glob('*xlsx')
names = []
for f in files:
    names.append(f.split(".")[0]) 
for x, y, name in zip(xs, ys, names):
    plt.scatter(x, y)
    plt.text(x, y, name)
plt.show()

"""***Hierarchical Clustering***"""

linkage_matrix = ward(cos_distance)
dendrogram(linkage_matrix, orientation='right', labels=names)
plt.tight_layout()
plt.show()

"""***K-Means***"""

from sklearn.cluster import KMeans
vectorizer = TfidfVectorizer()
km = KMeans(n_clusters=5, random_state=999)
cluster_solution = km.fit(dtm) 
cluster_membership = km.predict(dtm)
doc_distance_to_center = km.transform(dtm)
clusters = zip(cluster_membership, names)
top_docs = {'docs': names, 'cluster': cluster_membership,
            'doc_to_0':doc_distance_to_center[0:,0],
            'doc_to_1':doc_distance_to_center[0:,1],
            'doc_to_2':doc_distance_to_center[0:,2],
            'doc_to_3':doc_distance_to_center[0:,3],      
            'doc_to_4':doc_distance_to_center[0:,4],
           }
df = pd.DataFrame(top_docs)
df.sort_values(by=['cluster']) #sorting by clustering to find the similarity between different companies

"""*The above 3 algorithms yield similar result for the patent dataset and we can find that companies like **Google, Apple ,Sony Ericsson and Nokia** have a close similarity in their patents*

### Topic Modeling
"""

# function to display the topics
def display_topics(model, feature_names, n_top_words,model_name):
    topics = []
    for idx, topic in enumerate(model.components_):
        print("Topic %d:" % (idx))
        print(" ".join([feature_names[i]for i in topic.argsort()[:-n_top_words - 1:-1]]))
        topics.append(" ".join([feature_names[i]for i in topic.argsort()[:-n_top_words - 1:-1]]))
    compare_dict(topics,model_name)

# function to create a dict for analysing the topics found by different algorithms 
compare_topics = {}
def compare_dict(topics,model_name):
    compare_topics.update({model_name : topics})

"""***Non-negative matrix factorization (NMF)***"""

#Non-negative matrix factorization (NMF)
topic_words = []
n_features = 10000
num_topics= 5
num_top_words = 20

NMF_vectorizer = TfidfVectorizer(stop_words = 'english', max_df=0.95, min_df=2, max_features = n_features)
NMF_dtm = NMF_vectorizer.fit_transform(clean_corpus)
NMF_feature_names = NMF_vectorizer.get_feature_names()
NMF_clf = NMF(n_components = num_topics, random_state=999).fit(NMF_dtm)
display_topics(NMF_clf, NMF_feature_names, num_top_words,model_name = "NMF")

"""***Latent Dirichlet Allocation (LDA)***"""

lda_vectorizer = CountVectorizer(stop_words = 'english', max_df=0.95, min_df=2, max_features = n_features)
lda_dtm = lda_vectorizer.fit_transform(clean_corpus)
lda = LatentDirichletAllocation(n_components=num_topics,learning_method='batch', max_iter=5, random_state=999).fit(lda_dtm)
feature_names = lda_vectorizer.get_feature_names()
display_topics(lda, feature_names, num_top_words,model_name = "LDA")

"""***Latent Semantic Analysis/Indexing (LSA/LSI)***"""

lsi_vectorizer = CountVectorizer(max_df= 0.95, min_df = 2,stop_words='english', lowercase=True)
lsi_dtm = lsi_vectorizer.fit_transform(corpus)
lsi = TruncatedSVD(n_components= num_topics, n_iter = 5).fit(lsi_dtm)
feature_names = lsi_vectorizer.get_feature_names()
display_topics(lsi, feature_names, num_top_words,model_name = "LSI")

"""***Analysing the 5 topic found out by different algorithms***"""

dfcom=pd.DataFrame(compare_topics)
dfcom

"""*from the above comparison we can say that there is a **similarity** between the 5 topics found by all the 3 alogrithms*"""